{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omeraflalo/assigment-2-OCL/blob/master/assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZStr8Th4jBq"
      },
      "source": [
        "# Assignment 4: Ranking & Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN0aEQUD4mwy"
      },
      "source": [
        "## General guidelines\n",
        "\n",
        "This notebook contains considerable amount of code to help you complete this assignment. Your task is to implement any missing parts of the code and answer any questions (if exist) within this notebook. This will require understanding the existing code, may require reading about packages being used, reading additional resources, and maybe even going over your notes from class ðŸ˜±\n",
        "\n",
        "**Evaluation and auto-grading**: Your submissions will be evaluated using both automatic and manual grading. Code parts for implementation are marked with a comment `# YOUR CODE HERE`, and usually followed by cell(s) containing automatic tests that evaluate the correctness of your answer. Additionaly, staff will manually assess your submission. Any automatic tests that did not run due to your notebook timing out **will automatically receive 0 points**. The execution time excludes initial data download, which will already exist in the testing environment. The staff reserves the right to **modify any grade provided by the auto-grader** as well as to **execute additional tests not provided to you**. It is also important to note that **auto-graded cells only result in full or no credit**. In other words, you must pass all tests implemented in a test cell in order to get the credit for it, and passing some, but not all, of the tests in a test cell will not earn you any points for that cell. \n",
        "\n",
        "**Submission**: Unless specified otherwise, you need to upload this notebook file **with your ID as the file name**, e.g. 012345678.ipynb, to the assignment on Moodle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FLuP8v74n4i"
      },
      "source": [
        "# Tasks\n",
        "\n",
        "In this assignment, we are going to build a complete retrieval system using the inverted index (assignment 2)\n",
        "\n",
        "**Your tasks in this assignment are:**\n",
        "\n",
        "1. (40 Points) Implement ranking using TF-IDF and BM25.\n",
        "2. (35 Points) Implement search/retrieval using the inverted index. Implement compound ranking based on title and body. \n",
        "3. (25 Points) Using weighting of title and body scores. Provide three examples of mistakes that the model is making and explanations for why, and describe how you will change the model based on these observations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXsCKOx2FTn2"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "QGIQb5FHFSDH",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "bc6efd581a496afa7df97e9de2c5428b",
          "grade": false,
          "grade_id": "cell-f376d240eb48a4e1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import defaultdict,Counter\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "import numpy as np\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "import operator\n",
        "from itertools import islice,count\n",
        "from contextlib import closing\n",
        "\n",
        "import json\n",
        "from io import StringIO\n",
        "from pathlib import Path\n",
        "from operator import itemgetter\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiXcPL-9qH8g"
      },
      "source": [
        "## Tasks 1: Implement ranking using TF-IDF and BM25 (40 points).\n",
        "In this task, you need to implement TF-IDF and BM25. At this point, for simplicity, we do not use an inverted index. However, later on in this assignment, we will.\n",
        "\n",
        "Implementation remarks:\n",
        "* TF-IDF: use [sklearn TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). To deal with stopwords use the argument `stop_words`. (5 points)\n",
        "\n",
        "TfidfVectorizer suggests handling with additional parameters, as you can read in the documentation. Make sure you read about them.\n",
        "\n",
        "* Cosine Similarity: use [sklearn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html). (10 points)\n",
        "\n",
        "* BM25: Implement a BM25 version according to the provided skeleton **without the use of packages**. 15 points as follows:\n",
        "    * Prepare the data and filter stopwords. (5 points)\n",
        "    * Implement two functions at BM25 class. (10 points)\n",
        "\n",
        "* Ranking: implement topN functionallity (10 points)\n",
        "\n",
        "**Later in this assignment, we will write code for TF-IDF and BM25 that utilize the inverted index.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "pbkM4LjQzp1S",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "57b0e5963d640ba8b1fbcd0660c2522f",
          "grade": false,
          "grade_id": "cell-e99db25f2f734e3d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# set of documents\n",
        "data = ['The sky is blue and we can see the blue sun.',\n",
        "        'The sun is bright and yellow.',\n",
        "        'here comes the blue sun',\n",
        "        'Lucy in the sky with diamonds and you can see the sun in the sky',\n",
        "        'sun sun blue sun here we come',\n",
        "        'Lucy likes blue bright diamonds']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXG9bNEdqOZX"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-GxCnRuz4rE"
      },
      "source": [
        "**YOUR TASK (5 POINTS):**  Complete the implementation of `tf_idf_scores`, which calculates the tfidf for each word in a single document utilizing TfidfVectorizer via sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "NahU25YgsV8f",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e25ca815026e46063dc808bf7f65e14f",
          "grade": false,
          "grade_id": "cell-4bae717a683af9da",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def tf_idf_scores(data):\n",
        "    \"\"\"\n",
        "    This function calculates the tfidf for each word in a single document utilizing TfidfVectorizer via sklearn.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "      data: list of strings.\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "      Two objects as follows:\n",
        "                                a) DataFrame, documents as rows (i.e., 0,1,2,3, etc'), terms as columns ('bird','bright', etc').\n",
        "                                b) TfidfVectorizer object.\n",
        "\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    \n",
        "df_tfidfvect, tfidfvectorizer = tf_idf_scores(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "-P03jq5Z1POS",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1847af23ac4a83c7073569b3e5153d8b",
          "grade": true,
          "grade_id": "cell-8bc597f7193b750f",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#tests\n",
        "assert df_tfidfvect.shape[1] == 10 and df_tfidfvect.shape[0] == 6\n",
        "assert 'is' not in df_tfidfvect.columns and 'we' not in df_tfidfvect.columns\n",
        "assert 'sun' in df_tfidfvect.columns and 'yellow' in df_tfidfvect.columns\n",
        "assert round(df_tfidfvect.max(),3).max() == 0.798\n",
        "assert np.count_nonzero(df_tfidfvect) == 21\n",
        "assert type(tfidfvectorizer) == TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1DA10t1Fa4f"
      },
      "source": [
        "Now, upon existing TF-IDF matrix we are seeking for the similarity for given new queries. \n",
        "First we need to convert the **new** queries into a vector format. Therefore we use transform instead of fit_transform.\n",
        "\n",
        "Next, we would like to calculate the cosine similarity between queires and documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "l1mSzpDRxs2Q",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "97e9f39ab2011fa6bd9fa86ea7c637ed",
          "grade": false,
          "grade_id": "cell-b44922f34aff7c1d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "queries = ['look the the blue sky', 'He likes the blue the sun','Lucy likes blue sky with diamonds']\n",
        "queries_vector = tfidfvectorizer.transform(queries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDBJ24TQ8BD8"
      },
      "source": [
        "Now, lets calculate the cosine similarity utilizing sklearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RdQmZ-ZBFMT"
      },
      "source": [
        "**YOUR TASK (10 POINTS):**  Complete the implementation of `cosine_sim_using_sklearn`.\n",
        "You need to compute the similarity between the queries and the given documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "V_8kmozHFuOI",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8f3339c993c3615d613728da95de0b75",
          "grade": false,
          "grade_id": "cell-e829aaf7aa5730cd",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def cosine_sim_using_sklearn(queries,tfidf):\n",
        "    \"\"\"\n",
        "    In this function you need to utilize the cosine_similarity function from sklearn.\n",
        "    You need to compute the similarity between the queries and the given documents.\n",
        "    This function will return a DataFrame in the following shape: (# of queries, # of documents).\n",
        "    Each value in the DataFrame will represent the cosine_similarity between given query and document.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "      queries: sparse matrix represent the queries after transformation of tfidfvectorizer.\n",
        "      documents: sparse matrix represent the documents.\n",
        "      \n",
        "    Returns:\n",
        "    --------\n",
        "      DataFrame: This function will return a DataFrame in the following shape: (# of queries, # of documents).\n",
        "      Each value in the DataFrame will represent the cosine_similarity between given query and document.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "\n",
        "cosine_sim_df = cosine_sim_using_sklearn(queries_vector,df_tfidfvect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "snaKllK78uBn",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3ebf4bc2b1163a89fed8583d3c1a39f9",
          "grade": true,
          "grade_id": "cell-87cf8e377c2a7705",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# tests for cosine similarity\n",
        "assert cosine_sim_df.shape[0] == len(queries)\n",
        "assert cosine_sim_df.shape[1] == len(data)\n",
        "assert (abs(cosine_sim_df)>1).any().any() == False\n",
        "assert np.count_nonzero(cosine_sim_df) == 16\n",
        "assert round(cosine_sim_df.max(),3).max() == 0.888"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LFYms4iqRdY"
      },
      "source": [
        "### BM25\n",
        "In this section we will create a class of Best Match 25 (BM25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP_-2DvoU-mP"
      },
      "source": [
        "To use BM25 we will need to following parameters:\n",
        "\n",
        "* $k1$ and $b$ - free parameters\n",
        "* $f(t_i,D)$ - term frequency of term $t_i$ in document $D$\n",
        "* |$D$|- is the length of the document $D$ in terms \n",
        "* $avgdl$ -  average document length\n",
        "* $IDF$ - which is calculted as follows: $ln(\\frac{(N-n(t_i)+0.5}{n(t_i)+0.5)}+1)$, where $N$ is the total number of documents in the collection, and $n(t_i)$ is the number of documents containing $t_i$ (e.g., document frequency (df))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cmzp5vH1OneY"
      },
      "source": [
        "As a reminder of the data looks like this\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "p5-0bsOSYbeg",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "de9d7d8a1fdedaca984162c34336d551",
          "grade": false,
          "grade_id": "cell-b3f55758c498ba8c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhVa64REKdHU"
      },
      "source": [
        "**Note:** While using TF-IDF from sklearn we used the `stop_words` argument. \n",
        "\n",
        "**It is not the case when working with BM25 without any package.**\n",
        "Therefore, we need to filter the data and clean it. We will do so utilizing NLTK stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "V8cC5SngRpGF",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e51a444ee9f8bf086877ae2873f00990",
          "grade": false,
          "grade_id": "cell-48cfbc4a16de228b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "stopwords_frozen = frozenset(stopwords.words('english'))\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    text: string , represting the text to tokenize.    \n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    list of tokens (e.g., list of tokens).\n",
        "    \"\"\"\n",
        "    list_of_tokens =  [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in stopwords_frozen]    \n",
        "    return list_of_tokens\n",
        "\n",
        "\n",
        "clean_data = [tokenize(doc) for doc in data]\n",
        "clean_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHCcrCGYi5b8"
      },
      "source": [
        "Now let's find all the parameters needed for our toy example.\n",
        "**Later on we will need to gather this information from the inverted index.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSSOm4NwBVJT"
      },
      "source": [
        "**YOUR TASK (5 POINTS):**  Complete the implementation of `bm25_preprocess`.\n",
        "This function goes through the data and saves relevant information for the calculation of bm25. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "Il6NITZLZzfP",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "56478058d24b8acdaac5cc5717af53dc",
          "grade": false,
          "grade_id": "cell-f6b1b4fb8a11e44e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def bm25_preprocess(data):\n",
        "    \"\"\"\n",
        "    This function goes through the data and saves relevant information for the calculation of bm25. \n",
        "    Specifically, in this function, we will create 3 objects that gather information regarding document length, term frequency and\n",
        "    document frequency.\n",
        "    Parameters\n",
        "    -----------\n",
        "    data: list of lists. Each inner list is a list of tokens. \n",
        "    Example of data: \n",
        "    [\n",
        "        ['sky', 'blue', 'see', 'blue', 'sun'],\n",
        "        ['sun', 'bright', 'yellow'],\n",
        "        ['comes', 'blue', 'sun'],\n",
        "        ['lucy', 'sky', 'diamonds', 'see', 'sun', 'sky'],\n",
        "        ['sun', 'sun', 'blue', 'sun'],\n",
        "        ['lucy', 'likes', 'blue', 'bright', 'diamonds']\n",
        "    ]\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    three objects as follows:\n",
        "                a) doc_len: list of integer. Each element represents the length of a document.\n",
        "                b) tf: list of dictionaries. Each dictionary corresponds to a document as follows:\n",
        "                                                                    key: term\n",
        "                                                                    value: normalized term frequency (by the length of document)\n",
        "\n",
        "                                                                                               \n",
        "                c) df: dictionary representing the document frequency as follows:\n",
        "                                                                    key: term\n",
        "                                                                    value: document frequency\n",
        "    \"\"\"      \n",
        "    doc_len = []\n",
        "    tf = []\n",
        "    df = {}\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    return doc_len,tf,df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "LgPAqnBB_9bv",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "140bd96fa50f4733fbaee16fd70c97fa",
          "grade": true,
          "grade_id": "cell-132b971801797983",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# tests - preprocess data for naive bm25\n",
        "doc_len,tf,df = bm25_preprocess(clean_data)\n",
        "assert df['blue'] == 4\n",
        "assert sum(tf[0].values()) == 1\n",
        "assert sum(doc_len) > sum(df.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JmpJk7BC9N1"
      },
      "source": [
        "**YOUR TASK (10 POINTS):** Complete the implementation of `calc_idf` and `_score` at the BM25 class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "JjQUMtA5dFg4",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b43aa7ba807804cc806ede21d6f38a8a",
          "grade": false,
          "grade_id": "cell-660ceea782932b22",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class BM25:\n",
        "    \"\"\"\n",
        "    Best Match 25.\n",
        "\n",
        "    Parameters to tune\n",
        "    ----------\n",
        "    k1 : float, default 1.5\n",
        "\n",
        "    b : float, default 0.75\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    tf_ : list[dict[str, int]]\n",
        "        Term Frequency per document. So [{'hi': 1}] means\n",
        "        the first document contains the term 'hi' 1 time.\n",
        "        The frequnecy is normilzied by the max term frequency for each document.\n",
        "\n",
        "    doc_len_ : list[int]\n",
        "        Number of terms per document. So [3] means the first\n",
        "        document contains 3 terms. \n",
        "        \n",
        "    df_ : dict[str, int]\n",
        "        Document Frequency per term. i.e. Number of documents in the\n",
        "        corpus that contains the term.       \n",
        "\n",
        "    avg_doc_len_ : float\n",
        "        Average number of terms for documents in the corpus.\n",
        "\n",
        "    idf_ : dict[str, float]\n",
        "        Inverse Document Frequency per term.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,doc_len,df,tf=None,k1=1.5, b=0.75):\n",
        "        self.b = b\n",
        "        self.k1 = k1\n",
        "        self.tf_ = tf\n",
        "        self.doc_len_ = doc_len\n",
        "        self.df_ = df\n",
        "        self.N_ = len(doc_len)\n",
        "        self.avgdl_ = sum(doc_len) / len(doc_len)        \n",
        "        \n",
        "\n",
        "    def calc_idf(self,query):\n",
        "        \"\"\"\n",
        "        This function calculate the idf values according to the BM25 idf formula for each term in the query.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
        "        \n",
        "        Returns:\n",
        "        -----------\n",
        "        idf: dictionary of idf scores. As follows: \n",
        "                                                    key: term\n",
        "                                                    value: bm25 idf score\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "        \n",
        "\n",
        "    def search(self, queries):\n",
        "        \"\"\"\n",
        "        This function use the _score function to calculate the bm25 score for all queries provided.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        queries: list of lists. Each inner list is a list of tokens. For example:\n",
        "                                                                                    [\n",
        "                                                                                        ['look', 'blue', 'sky'],\n",
        "                                                                                        ['likes', 'blue', 'sun'],\n",
        "                                                                                        ['likes', 'diamonds']\n",
        "                                                                                    ]\n",
        "\n",
        "        Returns:\n",
        "        -----------\n",
        "        list of scores of bm25\n",
        "        \"\"\"\n",
        "        scores = []\n",
        "        for query in queries:            \n",
        "            scores.append([self._score(query, doc_id) for doc_id in range(self.N_)])\n",
        "        return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "Vxbt-DzED8I3",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "67f7927a0d65b44af7ad7782d59e16bb",
          "grade": false,
          "grade_id": "cell-e7075d260fd4067a",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class BM25(BM25):\n",
        "\n",
        "    def _score(self, query, doc_id):\n",
        "        \"\"\"\n",
        "        This function calculate the bm25 score for given query and document.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
        "        doc_id: integer, document id.\n",
        "        \n",
        "        Returns:\n",
        "        -----------\n",
        "        score: float, bm25 score.\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT49O3A_Peqv"
      },
      "source": [
        "Let's create a new instance of BM25, and calculte its score for all queries. \n",
        "Pay attetnion - we need to tokenize and filter the stopwords from the original queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "_5gTAhPtjKSO",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b5303fc020c13357f5d4d00cb75b8d50",
          "grade": false,
          "grade_id": "cell-fa65d259ff644369",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "bm25 = BM25(tf=tf,doc_len=doc_len,df=df)\n",
        "\n",
        "# Remove tokenizing and remove stopwords from queries\n",
        "clean_queries = [tokenize(query) for query in queries]\n",
        "BM25_res = bm25.search(clean_queries)\n",
        "BM25_df = pd.DataFrame(data = BM25_res,index = [query_id for query_id in range(len(clean_queries))] ,columns = [doc_id for doc_id in range(len(data))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "XGatJojrSiFw",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "12cf4c46da81e862891cecf3edad8c08",
          "grade": true,
          "grade_id": "cell-6218abc040ef663e",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# tests BM25\n",
        "assert BM25_df[0][0] == BM25_df[0][2]\n",
        "assert BM25_df[2][0] == BM25_df[2][2]\n",
        "assert BM25_df[3][0] != BM25_df[3][2]\n",
        "assert (BM25_df>1).any().any() == True\n",
        "assert (BM25_df==0).any().any() == True\n",
        "assert BM25_df.sum().argmax() == 5\n",
        "assert BM25_df.sum().argmin() == 1\n",
        "assert sum(bm25.search([['amit','livne'],['lucy','likes','blue','bright','diamonds']])[0]) == 0\n",
        "assert (bm25.search([['amit','livne'],['lucy','likes','blue','bright','diamonds']])[1][-1] > BM25_df[5][2])\n",
        "assert (bm25.search([['amit','livne'],['lucy','likes','blue','bright','diamonds']])[1][2] > BM25_df[1][2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPzSYz85THs2"
      },
      "source": [
        "We can now search for top-N documents for each query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1lkrsKyGVAF"
      },
      "source": [
        "**YOUR TASK (10 POINTS):**  Complete the implementation of `top_N_documents`, which sort and filter the top N docuemnts (by score) for each query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "NBsw59bxTHKu",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "823e2591f6053b77e7f0cfac0c20f1f7",
          "grade": false,
          "grade_id": "cell-d4de9e923dca0856",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def top_N_documents(df,N):\n",
        "    \"\"\"\n",
        "    This function sort and filter the top N docuemnts (by score) for each query.\n",
        "    \n",
        "    Parameters\n",
        "    ----------    \n",
        "    df: DataFrame (queries as rows, documents as columns)\n",
        "    N: Integer (how many document to retrieve for each query)    \n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "    top_N: dictionary is the following stracture:\n",
        "          key - query id.\n",
        "          value - sorted (according to score) list of pairs lengh of N. Eac pair within the list provide the following information (doc id, score)\n",
        "    \"\"\"    \n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "M1vqkZg_QQbR",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "11eb40559d8819bf0610774e00470047",
          "grade": true,
          "grade_id": "cell-1a4fe7778177237c",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# test - topN documents\n",
        "top_2_docs = top_N_documents(cosine_sim_df,2)\n",
        "top_10_docs = top_N_documents(cosine_sim_df,10)\n",
        "assert len(top_2_docs[0]) == 2\n",
        "assert len(top_2_docs[0][0]) == 2\n",
        "assert (top_2_docs[0][0][1] > top_N_documents(cosine_sim_df,2)[0][1][1])\n",
        "assert (top_10_docs[0][-1][-1] == 0)\n",
        "assert (top_10_docs[0][-1][-1] != top_10_docs[1][-1][-1])\n",
        "assert (top_10_docs[0][0][-1] != top_10_docs[1][0][-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcTOOAiTr_0s"
      },
      "source": [
        "The top-N functionality runs on all document and create a large DataFrame. What will happen if the number of documents is much larger? \n",
        "Will this function be efficient? \n",
        "\n",
        "Next, we will build an inverted index (like we did in assignment two). Then, we will use the inverted index utilizing the posting list to create a candidate list of documents per query. \n",
        "Therefore, we will narrow the documents relevant per query.\n",
        "\n",
        "Only then will we calculate the TF-IDF and cosine similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4GVs6-Tho-P"
      },
      "source": [
        "## Tasks 2: Implement compound ranking based on title and body using the inverted index (35 points).\n",
        "\n",
        "In order to do so, we will first perform parsing followed by building an Inverted index.\n",
        "Then we will create a searching and ranking mechanism.\n",
        "\n",
        "In this task, you will use Cranfield Corpus. But we need to do some setups in advance. \n",
        "\n",
        "**Setups:**\n",
        "\n",
        "*Make sure you upload the carnfield.py file. attached with this assignment*\n",
        "\n",
        "First, we will load it into the following data structures:\n",
        "\n",
        "* cran_txt_data - list of tuples. An example is provided below. Hold information regards the carnfield dataset.\n",
        "* cran_qry_rel_data - list of tuples. An example is provided below. Holds information regards the queries.\n",
        "\n",
        "We have already done all the load and preprocessing of the data for you.\n",
        "\n",
        "We split the queries data into the training and test sets.\n",
        "Next, we build two inverted indices (one built upon title information and the second on the text\\body of the documents). \n",
        "\n",
        "**After finishing the setups, we implement the ranking mechanism (TF-IDF and BM25 build upon inverted index).**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVFkJuuJnGQQ"
      },
      "source": [
        "### Cranfield Corpus\n",
        "\n",
        "You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/cran/).  <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wox5_m84nKdd"
      },
      "source": [
        "**Parsing**\n",
        "\n",
        "For detailed information about the parsing of this corpus look at [ this Notebook](https://colab.research.google.com/github/pragmalingu/experiments/blob/master/00_Data/CranfieldCorpus.ipynb) or, for parsing in general, read [this guide](https://pragmalingu.de/docs/guides/how-to-parse). An overview of the format of the files can be found here: [Data Sets](https://pragmalingu.de/docs/guides/data-comparison)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "dV4349DnFaXB",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d0b99b0ca55b027cc307d286f42f1c69",
          "grade": false,
          "grade_id": "cell-8b2b9d9fdd026022",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/cran/cran.tar.gz\n",
        "!tar -xf cran.tar.gz\n",
        "import carnfield\n",
        "\n",
        "cran_txt_data,cran_qry_rel_data = carnfield.main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stmEFWtVAXlb"
      },
      "source": [
        "### Data seperation\n",
        "In real-world scenarios, we do not know all the queries in advance. Thus, we are familiar with some queries and can use them to tune the parameters of our model (e.g., the k and b for BM25 or term that appears at least n times within the data, etc.'). We will split the queries into two sets to simulate this scenario: Train and Test.\n",
        "\n",
        "You can and should use the data from the train set to tune the parameters of your retrieval model. **You should not use the test data for tuning.**\n",
        "\n",
        "\n",
        "The test data should be used to evaluate your model according to the metrics in the following section (e.g., precision, recall etc.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk9882itsCaA"
      },
      "source": [
        "Example of cran_txt_data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "xre0ixPVrqqg",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d3c3746e3d29e039ffb0a72a69a5b27b",
          "grade": false,
          "grade_id": "cell-465458074bc926fe",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# list(islice(cran_txt_data.items(), 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfP-ttKPsGVT"
      },
      "source": [
        "Example of cran_qry_rel_data. \n",
        "For each query id you can see the question and the relevance assessments is a list of tuples of relevant document. The tuple structure is as follows: (document id, relevance score). Therefore a tuple such as (184,2) indicates that the document id 184 relevance score is 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "UUGuJu8ir0YZ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8337fa5fe64386c57a473e34974cb2cd",
          "grade": false,
          "grade_id": "cell-44e6f36ca78dfe12",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# list(islice(cran_qry_rel_data.items(), 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tftcouOuD4cS"
      },
      "source": [
        "Split to train and test queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fC0sqfVLAxwB",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "efd0c732a9b5b847d7fe62651111eca1",
          "grade": false,
          "grade_id": "cell-4148791d95352bb3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "cran_qry_rel_data_train = list(islice(cran_qry_rel_data.items(), 0,180))\n",
        "cran_qry_rel_data_test = list(islice(cran_qry_rel_data.items(), 180,225))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMFJey2QXWvp"
      },
      "source": [
        "Next, we will use the inverted index from assignment 2 (with some minor adaptations).\n",
        "We save a dictionary named `DL`, which fetches the document length of each document.\n",
        "\n",
        "Upon using the index functionality of add_doc, `DL` needs to be updated as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf028y32uxYx"
      },
      "source": [
        "### Inverted index (code from assignment 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_gmmMRIuzlR"
      },
      "source": [
        "#### Helper classes (code from assignment 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Z4wUkB8HuIfL",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "393cb64ada6c4d5f20e5cce0a8869297",
          "grade": false,
          "grade_id": "cell-8b7ad55b068c2c58",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Let's start with a small block size of 30 bytes just to test things out. \n",
        "BLOCK_SIZE = 30\n",
        "\n",
        "class MultiFileWriter:\n",
        "    \"\"\" Sequential binary writer to multiple files of up to BLOCK_SIZE each. \"\"\"\n",
        "    def __init__(self, base_dir, name):\n",
        "        self._base_dir = Path(base_dir)\n",
        "        self._name = name\n",
        "        self._file_gen = (open(self._base_dir / f'{name}_{i:03}.bin', 'wb') \n",
        "                          for i in count())\n",
        "        self._f = next(self._file_gen)\n",
        "    \n",
        "    def write(self, b):\n",
        "      locs = []\n",
        "      while len(b) > 0:\n",
        "        pos = self._f.tell()\n",
        "        remaining = BLOCK_SIZE - pos\n",
        "        # if the current file is full, close and open a new one.\n",
        "        if remaining == 0:  \n",
        "          self._f.close()\n",
        "          self._f = next(self._file_gen)\n",
        "          pos, remaining = 0, BLOCK_SIZE\n",
        "        self._f.write(b[:remaining])\n",
        "        locs.append((self._f.name, pos))\n",
        "        b = b[remaining:]\n",
        "      return locs\n",
        "\n",
        "    def close(self):\n",
        "      self._f.close()\n",
        "\n",
        "class MultiFileReader:\n",
        "  \"\"\" Sequential binary reader of multiple files of up to BLOCK_SIZE each. \"\"\"\n",
        "  def __init__(self):\n",
        "    self._open_files = {}\n",
        "\n",
        "  def read(self, locs, n_bytes):\n",
        "    b = []\n",
        "    for f_name, offset in locs:\n",
        "      if f_name not in self._open_files:\n",
        "        self._open_files[f_name] = open(f_name, 'rb')\n",
        "      f = self._open_files[f_name]\n",
        "      f.seek(offset)\n",
        "      n_read = min(n_bytes, BLOCK_SIZE - offset)\n",
        "      b.append(f.read(n_read))\n",
        "      n_bytes -= n_read\n",
        "    return b''.join(b)\n",
        "  \n",
        "  def close(self):\n",
        "    for f in self._open_files.values():\n",
        "      f.close()\n",
        "\n",
        "  def __exit__(self, exc_type, exc_value, traceback):\n",
        "    self.close()\n",
        "    return False "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "NoJlbdnHu2MR",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4bda607a9dc6eee47f23fa4f3722f477",
          "grade": false,
          "grade_id": "cell-bc28a7f345184ac7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "TUPLE_SIZE = 6       # We're going to pack the doc_id and tf values in this \n",
        "                     # many bytes.\n",
        "TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n",
        "\n",
        "DL = {}  # We're going to update and calculate this after each document. This will be usefull for the calculation of AVGDL (utilized in BM25) \n",
        "class InvertedIndex:  \n",
        "  def __init__(self, docs={}):\n",
        "    \"\"\" Initializes the inverted index and add documents to it (if provided).\n",
        "    Parameters:\n",
        "    -----------\n",
        "      docs: dict mapping doc_id to list of tokens\n",
        "    \"\"\"\n",
        "    # stores document frequency per term\n",
        "    self.df = Counter()\n",
        "    # stores total frequency per term\n",
        "    self.term_total = Counter()\n",
        "    # stores posting list per term while building the index (internally), \n",
        "    # otherwise too big to store in memory.\n",
        "    self._posting_list = defaultdict(list)\n",
        "    # mapping a term to posting file locations, which is a list of \n",
        "    # (file_name, offset) pairs. Since posting lists are big we are going to\n",
        "    # write them to disk and just save their location in this list. We are \n",
        "    # using the MultiFileWriter helper class to write fixed-size files and store\n",
        "    # for each term/posting list its list of locations. The offset represents \n",
        "    # the number of bytes from the beginning of the file where the posting list\n",
        "    # starts. \n",
        "    self.posting_locs = defaultdict(list)\n",
        "    \n",
        "    for doc_id, tokens in docs.items():\n",
        "      self.add_doc(doc_id, tokens)\n",
        "\n",
        "  def add_doc(self, doc_id, tokens):\n",
        "    \"\"\" Adds a document to the index with a given `doc_id` and tokens. It counts\n",
        "        the tf of tokens, then update the index (in memory, no storage \n",
        "        side-effects).\n",
        "    \"\"\"\n",
        "    DL[doc_id] = DL.get(doc_id,0) + (len(tokens))\n",
        "    w2cnt = Counter(tokens)\n",
        "    self.term_total.update(w2cnt)\n",
        "    max_value = max(w2cnt.items(), key=operator.itemgetter(1))[1]    \n",
        "    # frequencies = {key: value/max_value for key, value in frequencies.items()}\n",
        "    for w, cnt in w2cnt.items():        \n",
        "        self.df[w] = self.df.get(w, 0) + 1                \n",
        "        self._posting_list[w].append((doc_id, cnt))\n",
        "\n",
        "\n",
        "  def write(self, base_dir, name):\n",
        "    \"\"\" Write the in-memory index to disk and populate the `posting_locs`\n",
        "        variables with information about file location and offset of posting\n",
        "        lists. Results in at least two files: \n",
        "        (1) posting files `name`XXX.bin containing the posting lists.\n",
        "        (2) `name`.pkl containing the global term stats (e.g. df).\n",
        "    \"\"\"\n",
        "    #### POSTINGS ####\n",
        "    self.posting_locs = defaultdict(list)\n",
        "    with closing(MultiFileWriter(base_dir, name)) as writer:\n",
        "      # iterate over posting lists in lexicographic order\n",
        "      for w in sorted(self._posting_list.keys()):\n",
        "        self._write_a_posting_list(w, writer, sort=True)\n",
        "    #### GLOBAL DICTIONARIES ####\n",
        "    self._write_globals(base_dir, name)\n",
        "\n",
        "  def _write_globals(self, base_dir, name):\n",
        "    with open(Path(base_dir) / f'{name}.pkl', 'wb') as f:\n",
        "      pickle.dump(self, f)\n",
        "\n",
        "  def _write_a_posting_list(self, w, writer, sort=False):\n",
        "    # sort the posting list by doc_id\n",
        "    pl = self._posting_list[w]\n",
        "    if sort:\n",
        "      pl = sorted(pl, key=itemgetter(0))\n",
        "    # convert to bytes    \n",
        "    b = b''.join([(int(doc_id) << 16 | (tf & TF_MASK)).to_bytes(TUPLE_SIZE, 'big')\n",
        "                  for doc_id, tf in pl])\n",
        "    # write to file(s)\n",
        "    locs = writer.write(b)\n",
        "    # save file locations to index\n",
        "    self.posting_locs[w].extend(locs) \n",
        "\n",
        "  def __getstate__(self):\n",
        "    \"\"\" Modify how the object is pickled by removing the internal posting lists\n",
        "        from the object's state dictionary. \n",
        "    \"\"\"\n",
        "    state = self.__dict__.copy()\n",
        "    del state['_posting_list']\n",
        "    return state\n",
        "\n",
        "  @staticmethod\n",
        "  def read_index(base_dir, name):\n",
        "    with open(Path(base_dir) / f'{name}.pkl', 'rb') as f:\n",
        "      return pickle.load(f)\n",
        "\n",
        "  @staticmethod\n",
        "  def delete_index(base_dir, name):\n",
        "    path_globals = Path(base_dir) / f'{name}.pkl'\n",
        "    path_globals.unlink()\n",
        "    for p in Path(base_dir).rglob(f'{name}_*.bin'):\n",
        "      p.unlink()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHHVIZKaIhWP"
      },
      "source": [
        "In this assignment we will generate two indexes. \n",
        "One for titles and one for body (e.g., text).\n",
        "First, we need to tokenize the title and body textual information (e.g., text). \n",
        "Note: it can take several minutes to tokenize it.\n",
        "\n",
        "Next, we will create two seperated indexes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "eAweGxojIdFS",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5186ed75e1d4ab58614764d7f495bd40",
          "grade": false,
          "grade_id": "cell-242387ba2feef4ec",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "cran_txt_data_titles = {k: tokenize(v['title']) for k,v in cran_txt_data.items()}\n",
        "cran_txt_data_text = {k: tokenize(v['text']) for k,v in cran_txt_data.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8INaoSJEOD5"
      },
      "source": [
        "Next, we need to preprocess the text of the queries for both training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "MszY8ujbr58s",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "800bfb91f8c85bb303c007440edbee63",
          "grade": false,
          "grade_id": "cell-7964b532aec2f634",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "cran_txt_query_text_train = {q[0]: tokenize(q[1]['question']) for q in cran_qry_rel_data_train}\n",
        "cran_txt_query_text_test = {q[0]: tokenize(q[1]['question']) for q in cran_qry_rel_data_test}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whsU_r5bG8g9"
      },
      "source": [
        "#### Creating and writing the index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "aEOZhr7QvOMC",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "33b455bdeed31a70ccf5f62052ae73e2",
          "grade": false,
          "grade_id": "cell-d7ff0a9d1d1b54a7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "index_titles = InvertedIndex(docs=cran_txt_data_titles)\n",
        "index_text = InvertedIndex(docs=cran_txt_data_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Xt5ov8x5Bw8l",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e5643b4f783060cf8a770ca19d0f4e4a",
          "grade": false,
          "grade_id": "cell-3017a9ea682bee01",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# create directories for the different indices \n",
        "!mkdir body_index title_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "GJdYcsDvxVQ5",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "632ddd875ca8dbac3ca37ffa69bc951e",
          "grade": false,
          "grade_id": "cell-1d38f0893c9c0dab",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "index_titles.write('title_index','title')\n",
        "index_text.write('body_index','body')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFGBVH0uPISq"
      },
      "source": [
        "### Reading data from posting (code taken from assignment 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "xhMhnuBDQXRz",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "481335bfd98a294a74c7e0ce82ffd301",
          "grade": false,
          "grade_id": "cell-5248529b0b47fa61",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class InvertedIndex(InvertedIndex):\n",
        "  \n",
        "  def posting_lists_iter(self):\n",
        "    \"\"\" A generator that reads one posting list from disk and yields \n",
        "        a (word:str, [(doc_id:int, tf:int), ...]) tuple.\n",
        "    \"\"\"\n",
        "    with closing(MultiFileReader()) as reader:\n",
        "      for w, locs in self.posting_locs.items():\n",
        "        # read a certain number of bytes into variable b\n",
        "        b = reader.read(locs, self.df[w] * TUPLE_SIZE)\n",
        "        posting_list = []\n",
        "        # convert the bytes read into `b` to a proper posting list.\n",
        "        \n",
        "        for i in range(self.df[w]):\n",
        "          doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n",
        "          tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n",
        "          posting_list.append((doc_id, tf))\n",
        "        \n",
        "        yield w, posting_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROqsUP75WtPK"
      },
      "source": [
        "**Example of use**\n",
        "\n",
        "First, we will load the title index. \n",
        "\n",
        "Next, we will search for a given term in both lists (words, and pls)\n",
        "\n",
        "Remineder: pls is the information from the posting list.\n",
        "Each value within the `words` list has a corresponding value in the `pls` list.\n",
        "The value within a `pls` list is a list of tuples. Each tuple in the following format (x,y). Where x represent the document id, and `y` represent the occurence of the term in the document. \n",
        "\n",
        "In this example we are seeking the first term in the `words` list, and we can observe the is appears in 4 different documents. \n",
        "Moreover, we can observe that in document 807 it appears twice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5OjIGqGrRX87",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0561178852234ec48aa418ee788bbcf8",
          "grade": false,
          "grade_id": "cell-2b51b0e75b158093",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "idx_title = InvertedIndex.read_index('title_index', 'title')\n",
        "idx_body = InvertedIndex.read_index('body_index', 'body')\n",
        "# read posting lists from disk\n",
        "words, pls = zip(*idx_title.posting_lists_iter())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue6YmSy_yJoN"
      },
      "source": [
        "Let's check for example the document number `807`. \n",
        "We needed its document length (DL) which is suppose to be the length of its title + the length of the text of the document. \n",
        "Let's verify it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "zoH6o6VpSPg6",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3ae8ef32f2642293cf68a8fd0712dbae",
          "grade": false,
          "grade_id": "cell-671850a4c71e991a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#Let's check the title of document 807\n",
        "cran_txt_data_titles['807']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PizC-65xpzvk"
      },
      "source": [
        "Clearly `000` should not appear twice.\n",
        "Let have a second look on the original document and understand weather it's a mistake within a tokenization step or in the original title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "KDFB8nfsp-x8",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "15bb296b933a3cb18cf28760739a677c",
          "grade": false,
          "grade_id": "cell-e8276f6ab0133ecc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "cran_txt_data['807']['title']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv1fmK21qc2c"
      },
      "source": [
        "As we could have guessed, the token `000` does not appear twice in this document.\n",
        "\n",
        "**Reminder: garbage in = garbage out.**\n",
        "\n",
        "**For this assignment, it is ok to leave it as is. However, verify this part when you are working on your project.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "NavtokcsyJDs",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "21afb46320cf95d13978390385500980",
          "grade": false,
          "grade_id": "cell-36d9d323ca5c7e21",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert DL['807'] == len(cran_txt_data_titles['807']) + len(cran_txt_data_text['807'])\n",
        "assert len(DL) == len(cran_txt_data_titles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCgSReAUwG-M"
      },
      "source": [
        "### Ranking\n",
        "In this section, you will be given a query or queries and return a ranked list of documents to retrieve.\n",
        "In this assignment, we are experimenting with two methods. TF-IDF and BM25. \n",
        "\n",
        "**We will use the inverted index in order to do so and will not utilize the whole corpus in advance.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBlLqQii5f6B"
      },
      "source": [
        "#### TF-IDF for carnfield data (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1qulheONwg8"
      },
      "source": [
        "Bellow cells contain the following functions: \n",
        "\n",
        "*   *generate_query_tfidf_vector* - Generate a vector representing the query\n",
        "*   *get_candidate_documents_and_scores* - Generate a dictionary representing a pool of candidate documents for a given query.\n",
        "*   *generate_document_tfidf_matrix* - Generate a DataFrame of tfidf scores for a given query.\n",
        "*   *cosine_similarity* - Calculate the cosine similarity for each candidate document in D and a given query (e.g., Q). **You will impelemnt this function (10 points)**\n",
        "\n",
        "*   *get_top_n* - Sort and return the highest N documents according to the cosine similarity score.\n",
        "\n",
        "*   *get_topN_score_for_queries* - Generate a dictionary that gather for every query its topN score. **You will impelemnt this function (10 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "mbPJ5YxcXRcT",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7aedea7913856a9082c9c0bc9a338f0c",
          "grade": false,
          "grade_id": "cell-e244d9f201109308",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def generate_query_tfidf_vector(query_to_search,index):\n",
        "    \"\"\" \n",
        "    Generate a vector representing the query. Each entry within this vector represents a tfidf score.\n",
        "    The terms representing the query will be the unique terms in the index.\n",
        "\n",
        "    We will use tfidf on the query as well. \n",
        "    For calculation of IDF, use log with base 10.\n",
        "    tf will be normalized based on the length of the query.    \n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n",
        "                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n",
        "\n",
        "    index:           inverted index loaded from the corresponding files.    \n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    vectorized query with tfidf scores\n",
        "    \"\"\"\n",
        "    \n",
        "    epsilon = .0000001\n",
        "    total_vocab_size = len(index.term_total)\n",
        "    Q = np.zeros((total_vocab_size))\n",
        "    term_vector = list(index.term_total.keys())    \n",
        "    counter = Counter(query_to_search)\n",
        "    for token in np.unique(query_to_search):\n",
        "        if token in index.term_total.keys(): #avoid terms that do not appear in the index.               \n",
        "            tf = counter[token]/len(query_to_search) # term frequency divded by the length of the query\n",
        "            df = index.df[token]            \n",
        "            idf = math.log((len(DL))/(df+epsilon),10) #smoothing\n",
        "            \n",
        "            try:\n",
        "                ind = term_vector.index(token)\n",
        "                Q[ind] = tf*idf                    \n",
        "            except:\n",
        "                pass\n",
        "    return Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Hwf5oq558hNh",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "13ba9f2e92bb01e6ab6e71368ce3237a",
          "grade": false,
          "grade_id": "cell-f6349be47be84f49",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def get_posting_iter(index):\n",
        "    \"\"\"\n",
        "    This function returning the iterator working with posting list.\n",
        "    \n",
        "    Parameters:\n",
        "    ----------\n",
        "    index: inverted index    \n",
        "    \"\"\"\n",
        "    words, pls = zip(*index.posting_lists_iter())\n",
        "    return words,pls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "HhQRw9ye10r1",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fcbd55876e4aab5ad2954edd94f0de80",
          "grade": false,
          "grade_id": "cell-efd15bbca288c498",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def get_candidate_documents_and_scores(query_to_search,index,words,pls):\n",
        "    \"\"\"\n",
        "    Generate a dictionary representing a pool of candidate documents for a given query. This function will go through every token in query_to_search\n",
        "    and fetch the corresponding information (e.g., term frequency, document frequency, etc.') needed to calculate TF-IDF from the posting list.\n",
        "    Then it will populate the dictionary 'candidates.'\n",
        "    For calculation of IDF, use log with base 10.\n",
        "    tf will be normalized based on the length of the document.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n",
        "                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n",
        "\n",
        "    index:           inverted index loaded from the corresponding files.\n",
        "\n",
        "    words,pls: iterator for working with posting.\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    dictionary of candidates. In the following format:\n",
        "                                                               key: pair (doc_id,term)\n",
        "                                                               value: tfidf score. \n",
        "    \"\"\"\n",
        "    candidates = {}\n",
        "    for term in np.unique(query_to_search):\n",
        "        if term in words:            \n",
        "            list_of_doc = pls[words.index(term)]            \n",
        "            normlized_tfidf = [(doc_id,(freq/DL[str(doc_id)])*math.log(len(DL)/index.df[term],10)) for doc_id, freq in list_of_doc]\n",
        "            \n",
        "            for doc_id, tfidf in normlized_tfidf:\n",
        "                candidates[(doc_id,term)] = candidates.get((doc_id,term), 0) + tfidf               \n",
        "\n",
        "    return candidates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "11pKF-MqFhAt",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "debdd77e8edf03f9341f2a994c1cf339",
          "grade": false,
          "grade_id": "cell-7dc5f8953f2e0523",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def generate_document_tfidf_matrix(query_to_search,index,words,pls):\n",
        "    \"\"\"\n",
        "    Generate a DataFrame `D` of tfidf scores for a given query. \n",
        "    Rows will be the documents candidates for a given query\n",
        "    Columns will be the unique terms in the index.\n",
        "    The value for a given document and term will be its tfidf score.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n",
        "                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n",
        "\n",
        "    index:           inverted index loaded from the corresponding files.\n",
        "\n",
        "    \n",
        "    words,pls: iterator for working with posting.\n",
        "\n",
        "    Returns:\n",
        "    -----------\n",
        "    DataFrame of tfidf scores.\n",
        "    \"\"\"\n",
        "    \n",
        "    total_vocab_size = len(index.term_total)\n",
        "    candidates_scores = get_candidate_documents_and_scores(query_to_search,index,words,pls) #We do not need to utilize all document. Only the docuemnts which have corrspoinding terms with the query.\n",
        "    unique_candidates = np.unique([doc_id for doc_id, freq in candidates_scores.keys()])\n",
        "    D = np.zeros((len(unique_candidates), total_vocab_size))\n",
        "    D = pd.DataFrame(D)\n",
        "    \n",
        "    D.index = unique_candidates\n",
        "    D.columns = index.term_total.keys()\n",
        "\n",
        "    for key in candidates_scores:\n",
        "        tfidf = candidates_scores[key]\n",
        "        doc_id, term = key    \n",
        "        D.loc[doc_id][term] = tfidf\n",
        "\n",
        "    return D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yk_yJnzM5Tb"
      },
      "source": [
        "**YOUR TASK (10 POINTS):** Complete the implementation of `cosine_similarity`. This function calculate the cosine similarity for each candidate document in D and a given query (e.g., Q) and return a dictionary of cosine similary scores.\n",
        "\n",
        "**Note:** for this task you cannot use sklearn. However, you can use pandas or numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "92OO-c4Ah7TJ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5764473e9075707e08292f35a6680eb7",
          "grade": false,
          "grade_id": "cell-b87e721c86d6850d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(D,Q):\n",
        "    \"\"\"\n",
        "    Calculate the cosine similarity for each candidate document in D and a given query (e.g., Q).\n",
        "    Generate a dictionary of cosine similarity scores \n",
        "    key: doc_id\n",
        "    value: cosine similarity score\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    D: DataFrame of tfidf scores.\n",
        "\n",
        "    Q: vectorized query with tfidf scores\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    dictionary of cosine similarity score as follows:\n",
        "                                                                key: document id (e.g., doc_id)\n",
        "                                                                value: cosine similarty score.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Yh7H2unw9oTf",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e1148d91409291dfe74053192176ddff",
          "grade": false,
          "grade_id": "cell-5bad3528aaeb6a2f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def get_top_n(sim_dict,N=3):\n",
        "    \"\"\" \n",
        "    Sort and return the highest N documents according to the cosine similarity score.\n",
        "    Generate a dictionary of cosine similarity scores \n",
        "   \n",
        "    Parameters:\n",
        "    -----------\n",
        "    sim_dict: a dictionary of similarity score as follows:\n",
        "                                                                key: document id (e.g., doc_id)\n",
        "                                                                value: similarity score. We keep up to 5 digits after the decimal point. (e.g., round(score,5))\n",
        "\n",
        "    N: Integer (how many documents to retrieve). By default N = 3\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    a ranked list of pairs (doc_id, score) in the length of N.\n",
        "    \"\"\"\n",
        "    \n",
        "    return sorted([(doc_id,round(score,5)) for doc_id, score in sim_dict.items()], key = lambda x: x[1],reverse=True)[:N]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziJUOZwrMTiL"
      },
      "source": [
        "**YOUR TASK (10 POINTS)**: Complete the implementation of `get_topN_score_for_queries`. This function generate a dictionary that gather for every query its topN score, based on cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "2AJ1qn2YVpN5",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1c2ad4ade44cc22dd440bf53b0e6f5d8",
          "grade": false,
          "grade_id": "cell-59c3035e52c567f5",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def get_topN_score_for_queries(queries_to_search,index,N=3):\n",
        "    \"\"\" \n",
        "    Generate a dictionary that gathers for every query its topN score.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    queries_to_search: a dictionary of queries as follows: \n",
        "                                                        key: query_id\n",
        "                                                        value: list of tokens.\n",
        "    index:           inverted index loaded from the corresponding files.    \n",
        "    N: Integer. How many documents to retrieve. This argument is passed to the topN function. By default N = 3, for the topN function. \n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    return: a dictionary of queries and topN pairs as follows:\n",
        "                                                        key: query_id\n",
        "                                                        value: list of pairs in the following format:(doc_id, score). \n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "oxYOsSwMWDmo",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "10bb0b3ec5fdc3580a3f16ff24ebe489",
          "grade": false,
          "grade_id": "cell-9c1d5b562ad2081e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "tfidf_queries_score_train = get_topN_score_for_queries(cran_txt_query_text_train,idx_title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "F_fNI7HWQaSm",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "801769ae0a51e8c76da05cc19c93670e",
          "grade": true,
          "grade_id": "cell-3b535d59edd83610",
          "locked": true,
          "points": 20,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#tests \n",
        "assert len(tfidf_queries_score_train)==180\n",
        "assert 0 not in tfidf_queries_score_train.keys()\n",
        "assert len(tfidf_queries_score_train[5])==3\n",
        "assert tfidf_queries_score_train[172][0][1] == tfidf_queries_score_train[172][1][1]\n",
        "assert tfidf_queries_score_train[14][0][1] == tfidf_queries_score_train[172][1][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox7l1WIMatYa"
      },
      "source": [
        "For query 172 we can observe two document with cosine similarity score of 1. Let's have a glance on this query and documents for making sure it makes sense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "6yB6gHojaN77",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "211e712289f49b0efe56ce97f3db7100",
          "grade": false,
          "grade_id": "cell-1169674deaad07c6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "print('relevnt documents and tfidf score for query number 172 :',tfidf_queries_score_train[172])\n",
        "print('query: ' ,cran_txt_query_text_train[172])\n",
        "print('docuemnt 320: ', cran_txt_data_titles['320'])\n",
        "print('docuemnt 320: ', cran_txt_data_titles['321'])\n",
        "print('docuemnt 322: ', cran_txt_data_titles['322'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swC-nUdwviXN"
      },
      "source": [
        "#### BM25 for carnfield data (15 points)\n",
        "As a reminder:\n",
        "\n",
        "To use BM25 we will need to following parameters:\n",
        "\n",
        "* $k1$ and $b$ - free parameters\n",
        "* $f(t_i,D)$ - term frequency of term $t_i$ in document $D$\n",
        "* |$D$|- is the length of the document $D$ in terms \n",
        "* $avgdl$ -  average document length\n",
        "* $IDF$ - which is calculted as follows: $ln(\\frac{(N-n(t_i)+0.5}{n(t_i)+0.5)}+1)$, where $N$ is the total number of documents in the collection, and $n(t_i)$ is the number of documents containing $t_i$ (e.g., document frequency (df)).\n",
        "\n",
        "Now, we will use the inverted index to fetch this information.\n",
        "\n",
        "**We need to check only documents which are 'candidates' for a given query.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqf4UeRPTUQm"
      },
      "source": [
        "**YOUR TASK (15 POINTS):** Complete the implementation of `search` at the BM25 class.\n",
        "Differently, from previous implememntation, this time you should check only documents which are 'candidates' for a given query. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "nPOS5HoohGTJ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d622e6d162d317bde0c016e385409d48",
          "grade": false,
          "grade_id": "cell-e55a7a9f6855d2a6",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from itertools import chain\n",
        "import time\n",
        "# When preprocessing the data have a dictionary of document length for each document saved in a variable called `DL`.\n",
        "class BM25_from_index:\n",
        "    \"\"\"\n",
        "    Best Match 25.    \n",
        "    ----------\n",
        "    k1 : float, default 1.5\n",
        "\n",
        "    b : float, default 0.75\n",
        "\n",
        "    index: inverted index\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,index,k1=1.5, b=0.75):\n",
        "        self.b = b\n",
        "        self.k1 = k1\n",
        "        self.index = index\n",
        "        self.N = len(DL)\n",
        "        self.AVGDL = sum(DL.values())/self.N\n",
        "        self.words, self.pls = zip(*self.index.posting_lists_iter())        \n",
        "\n",
        "    def calc_idf(self,list_of_tokens):\n",
        "        \"\"\"\n",
        "        This function calculate the idf values according to the BM25 idf formula for each term in the query.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
        "        \n",
        "        Returns:\n",
        "        -----------\n",
        "        idf: dictionary of idf scores. As follows: \n",
        "                                                    key: term\n",
        "                                                    value: bm25 idf score\n",
        "        \"\"\"        \n",
        "        idf = {}        \n",
        "        for term in list_of_tokens:            \n",
        "            if term in self.index.df.keys():\n",
        "                n_ti = self.index.df[term]\n",
        "                idf[term] = math.log(1 + (self.N - n_ti + 0.5) / (n_ti + 0.5))\n",
        "            else:\n",
        "                pass                             \n",
        "        return idf\n",
        "        \n",
        "\n",
        "    def search(self, queries,N=3):\n",
        "        \"\"\"\n",
        "        This function calculate the bm25 score for given query and document.\n",
        "        We need to check only documents which are 'candidates' for a given query. \n",
        "        This function return a dictionary of scores as the following:\n",
        "                                                                    key: query_id\n",
        "                                                                    value: a ranked list of pairs (doc_id, score) in the length of N.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
        "        doc_id: integer, document id.\n",
        "        \n",
        "        Returns:\n",
        "        -----------\n",
        "        score: float, bm25 score.\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _score(self, query, doc_id):\n",
        "        \"\"\"\n",
        "        This function calculate the bm25 score for given query and document.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
        "        doc_id: integer, document id.\n",
        "        \n",
        "        Returns:\n",
        "        -----------\n",
        "        score: float, bm25 score.\n",
        "        \"\"\"        \n",
        "        score = 0.0        \n",
        "        doc_len = DL[str(doc_id)]        \n",
        "             \n",
        "        for term in query:\n",
        "            if term in self.index.term_total.keys():                \n",
        "                term_frequencies = dict(self.pls[self.words.index(term)])                \n",
        "                if doc_id in term_frequencies.keys():            \n",
        "                    freq = term_frequencies[doc_id]\n",
        "                    numerator = self.idf[term] * freq * (self.k1 + 1)\n",
        "                    denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.AVGDL)\n",
        "                    score += (numerator / denominator)\n",
        "        return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "51fbd11cd4366bc937dbfdb776e1d39a",
          "grade": false,
          "grade_id": "cell-091c53ed5e95592e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "LDmyUm1KM03j"
      },
      "outputs": [],
      "source": [
        "bm25_title = BM25_from_index(idx_title)\n",
        "bm25_queries_score_train = bm25_title.search(cran_txt_query_text_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "zQXVP_4MUeuN",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "984ca7b5c07667aeca49b9b6bac323bc",
          "grade": true,
          "grade_id": "cell-d01c7bc7be812e3f",
          "locked": true,
          "points": 15,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#tests\n",
        "assert len(bm25_queries_score_train)==180\n",
        "assert 0 not in bm25_queries_score_train.keys()\n",
        "assert len(bm25_queries_score_train[5])==3\n",
        "assert bm25_queries_score_train[172][0][1] != bm25_queries_score_train[172][1][1]\n",
        "assert bm25_queries_score_train[14][0][1] != bm25_queries_score_train[172][1][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF6VVM9KHC1N"
      },
      "source": [
        "## Task 3: Using weights of title and body scores (25 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvJcS4AfHkT5"
      },
      "source": [
        "Reminder: we are building on the training set.\n",
        "Later you need to optimize the parameters and run the best parameters on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek3G_fS61s8w"
      },
      "source": [
        "Now we will experiment with two sets of results. \n",
        "The first corresponds to results from the title index. \n",
        "The second corresponds to the results from the body index.\n",
        "\n",
        "We need to merge them into a single result set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY9tftQ9WNn0"
      },
      "source": [
        "**YOUR TASK (15 POINTS):** Complete the implementation of `merge_results`.\n",
        "This function merge and sort documents retrieved by its weighte score (e.g., title and body).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "Oz1yjX5z2MmJ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "255498d4034e5373b2c3591fe77e8f47",
          "grade": false,
          "grade_id": "cell-12bfb07aef0d4308",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def merge_results(title_scores,body_scores,title_weight=0.5,text_weight=0.5,N = 3):    \n",
        "    \"\"\"\n",
        "    This function merge and sort documents retrieved by its weighte score (e.g., title and body). \n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    title_scores: a dictionary build upon the title index of queries and tuples representing scores as follows: \n",
        "                                                                            key: query_id\n",
        "                                                                            value: list of pairs in the following format:(doc_id,score)\n",
        "                \n",
        "    body_scores: a dictionary build upon the body/text index of queries and tuples representing scores as follows: \n",
        "                                                                            key: query_id\n",
        "                                                                            value: list of pairs in the following format:(doc_id,score)\n",
        "    title_weight: float, for weigted average utilizing title and body scores\n",
        "    text_weight: float, for weigted average utilizing title and body scores\n",
        "    N: Integer. How many document to retrieve. This argument is passed to topN function. By default N = 3, for the topN function. \n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    dictionary of querires and topN pairs as follows:\n",
        "                                                        key: query_id\n",
        "                                                        value: list of pairs in the following format:(doc_id,score). \n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "BPrpxaW1WtPF",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "71eef6e9777f134cdb8df964565dae2d",
          "grade": false,
          "grade_id": "cell-eafc1d1e97e87f62",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "bm25_title = BM25_from_index(idx_title)\n",
        "bm25_body = BM25_from_index(idx_body)\n",
        "\n",
        "bm25_queries_score_train_title = bm25_title.search(cran_txt_query_text_train)\n",
        "bm25_queries_score_train_body = bm25_body.search(cran_txt_query_text_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "xHD8GBBZX_RL",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7676da02ce4147d45fedb9a118f1fe97",
          "grade": true,
          "grade_id": "cell-78bf3238907c44fa",
          "locked": true,
          "points": 15,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#tests\n",
        "w1,w2 = 0.5, 0.5\n",
        "w3,w4 = 0.25,0.75\n",
        "\n",
        "half_and_half = merge_results(bm25_queries_score_train_title,bm25_queries_score_train_body,w1,w2)        \n",
        "assert len(half_and_half[2]) == 3\n",
        "assert type(half_and_half) == dict\n",
        "assert type(half_and_half[2]) == list\n",
        "assert len(half_and_half) == 180\n",
        "assert half_and_half[2][0][1] == 0.5 * (bm25_queries_score_train_title[2][-1][1]+ bm25_queries_score_train_body[2][0][1])\n",
        "\n",
        "quarter_and_three_quarters = merge_results(bm25_queries_score_train_title,bm25_queries_score_train_body,0.25,0.75)        \n",
        "\n",
        "assert quarter_and_three_quarters[2][0][1] == (w3 * bm25_queries_score_train_title[2][-1][1] + w4 * bm25_queries_score_train_body[2][0][1])\n",
        "assert {k for k,v in half_and_half[16]} != {k for k,v in quarter_and_three_quarters[16]}\n",
        "assert len({k for k,v in half_and_half[16]}.union({k for k,v in quarter_and_three_quarters[16]})) < (len({k for k,v in half_and_half[16]}) + len({k for k,v in quarter_and_three_quarters[16]}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "finBStQSEfQO"
      },
      "source": [
        "**YOUR TASK (10 POINTS):** provide three examples of mistakes that the model is making and explanations for why, and describe how you will change the model based on these observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "AyUzoE--JLjq",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8b41efe1b8a8553eb34c61962061fd7c",
          "grade": true,
          "grade_id": "cell-72de9207dbdfe493",
          "locked": false,
          "points": 10,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": false,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}